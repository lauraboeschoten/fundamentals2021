---
title: "Practice Exam"
author: "Gerko Vink"
date: "Fundamental Techniques in Datascience with R"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
runtime: shiny_prerendered
---

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
    font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

---

```{r setup, include=FALSE}
library(learnr)
```

This is a practice exam. This exam does not cover the complete course materials, but gives you an indication of the type and format of questions you may expect. 

---

# Multiple Choice 

---

## Question 1

```{r q1, echo=FALSE}
question("The standard error of a regression weight indicates",
  answer("The standard deviation of the sampling distribution of that regression weight", correct = TRUE),
  answer("The standard deviation of the sampling distribution of that predicted outcome", message = "This is not true. There could be multiple predictors that would each contribute to the outcome. Each of those predictors have a seperate standard error. Moreover, the uncertainty about a regression weight may be large while the uncertainty about the predicted outcome is very small."),
  answer("The variance of the sampling distribution of that regression weight", message = "It is the standard deviation"),
  answer("The variance of the sampling distribution of predicted outcome", message = "It is the standard deviation of the sampling distribution of that regression weight. There could be multiple predictors that would each contribute to the outcome. Each of those predictors have a seperate standard error. Moreover, the uncertainty about a regression weight may be large while the uncertainty about the predicted outcome is very small."),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
``` 

---

## Question 2
```{r echo=FALSE}
library(magrittr)
anscombe %$% lm(y1 ~ x1) %>% summary
```

---

```{r q2, echo=FALSE}
question("In the above regression output, which parameter(s) would allow you to inspect normality of residuals",
   answer("The Residual standard error: 1.237 with the 9 degrees of freedom", message = "The residual standard error gives you an indication about the spread of the residuals. The lower the residual standard error, the more similar the residuals"),
  answer("The regression coefficients (Estimate and Std. Error)", message = "The regression coefficients give you the estimated effect of the predictor on the predicted outcome. The standard error gives you an estimate for the standard deviation of the sampling distribution of the estimated effect"),
  answer("The distributional parameters for the residuals (Min, 1Q, Median, 3Q and Max)", correct = TRUE),
  answer("The Anova parameters F-statistic: 17.99 on 1 and 9 DF", message = "The Anova tests the overall model. That is, it tests if the model explains more than the residuals."),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
```

---

## Question 3
```{r q3, echo=FALSE}
question("Would you consider the model y1 ~ x1 to be a good model?",
   answer("Yes, because the F-statistic is significant F(1, 9) = 17.99, p = .00217",  message = "For this specific output I would count this answer as correct too. But beware. It may be that the ANOVA or regression weight are only significant because of a large sample size. In such cases goodness of fit can only be interpreted by studying the R-squared for models with 1 predictor and the Adjusted R-squared for models with more than one predictor"),
  answer("Yes, because the regression weight for x1 is significant t(1) = 4.241, p = .00217", message = "For this specific output I would count this answer as correct too. But beware. It may be that the ANOVA or regression weight are only significant because of a large sample size. In such cases goodness of fit can only be interpreted by studying the R-squared for models with 1 predictor and the Adjusted R-squared for models with more than one predictor"),
  answer("Yes because the residual variance is lower than the intercept 1.237 < 3.001", message = "False. This answer is absolute nonsense."),
  answer("Yes because the R-squared is R2 = .6665", correct = TRUE),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
```

---

## Question 4
```{r q4, echo=FALSE}
question("What is not an assumption for logistic regression",
   answer("The logodds of the outcome have a linear relation with the predictors"),
  answer("The observations are independent of each other"),
  answer("The residuals have constant variance", correct = TRUE),
  answer("The predictors are not too highly correlated"),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
```

---

## Question 5
```{r q5, echo=FALSE}
question("Why would we preferably avoid multicollinearity",
   answer("Because the predicted outcome will be biased", message = "The predicted outcome is not affected. Only the estimates are wrong"),
  answer("Because the p-values will be too low", message = "p-values will be larger due to the added uncertainty on how to distribute the effect over the highly correlated predictors"),
  answer("Because our regression estimates will be biased", correct = TRUE),
  answer("Because it will make the regression line nonlinear", message = "Multicollinearity affects the parameter estimates. The line these parameters are based on will still be a linear approximation" ),
  allow_retry = TRUE,
  random_answer_order = TRUE
)
```

---

# Open questions

## Question 6
**What do we test with crossvalidation?**


```{r filter, exercise=TRUE}
# youranswer
```

```{r filter-solution}
# With crossvalidation we test if the results of our model would generalize to 
# an independent data set. In other words, we test if the results would also 
# hold for another data set that comes from the same true data generating model 
# - usually a population. If we don't use crossvalidation we might miss that our
# model would only fit to our data set. 
```

---

## Question 7

**Use the output below to calculate a predicted `bmi` for a boy with the following parameters:**

- `age = 14` years of age
- `height = 153` cm tall
- `wgt = 52` kg
- `reg = "south"`

```{r echo = FALSE, message=FALSE}
library(mice)
fit <- boys %$% lm(bmi ~ age + hgt * wgt + reg)
coef(fit) 
```

```{r q7, exercise=TRUE}
# youranswer
```

```{r q7-solution}
#The boy's predicted bmi is: 23.14209

19.177830409 + (14 * -0.087653081) + (153 * -0.111429261) + 
(55 * 0.762721904) + -0.116025646 + (153 * 55  * -0.002328413)

# You have to multiply all relevant estimates with the boy's value. For the 
# interaction wgt * hgt you need to multiply both wgt and hgt
```

For the exam you will not need a calculator. If you need to calculate some predictions, the regression estimates will be simple numbers that you can multiply by head. 