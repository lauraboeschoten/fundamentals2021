---
title: "Exercise 6"
author: "Gerko Vink"
date: "Fundamental Techniques in Data Science with R"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    number_sections: false
---
  
<style type="text/css">
  
body{ /* Normal  */
  font-size: 12px;
  }
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 18px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 18px;
}
h2 { /* Header 2 */
  font-size: 18px;
}
h3 { /* Header 3 */
  font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
  font-size: 14px;
}
</style>
  
---

```{r echo=FALSE}
printAnswers <- TRUE
printAnswers2 <- FALSE
```

We use the following packages in this Practical:
```{r message=FALSE, warning = FALSE}
library(dplyr)
library(magrittr)
library(ggplot2)
```

In this practical you will need to perform regression analysis and create plots with ggplot2. I give you some examples and ask from you to apply the techniques I demonstrate. For some exercises I give you the solution (e.g. the resulting graph) and the interpretation. The exercise is then to provide to me the code that generates the solution and give me the interpretation for the exercises where this is omitted. 

Feel free to ask me, if you have questions. 

All the best, 

Gerko

---

# Models and output

---

1. **Fit the following linear models on the anscombe data:**

- `y1` predicted by `x1` - stored in object `fit1`
- `y2` predicted by `x2` - stored in object `fit2`
- `y3` predicted by `x3` - stored in object `fit3`
- `y4` predicted by `x4` - stored in object `fit4`

I give you the code for first regression model. You need to fit the other three models yourself. 
```{r}
fit1 <- anscombe %$%
  lm(y1 ~ x1)
```
```{r echo = printAnswers}
fit2 <- anscombe %$%
  lm(y2 ~ x2)
fit3 <- anscombe %$%
  lm(y3 ~ x3)
fit4 <- anscombe %$%
  lm(y4 ~ x4)
```

---

2. **`Display a data frame with the coefficients of the 4 fitted objects from Exercise 1**

Use the following code to markup your output into a nice format
```{r}
output <- data.frame(fit1 = coef(fit1),
                     fit2 = coef(fit2),
                     fit3 = coef(fit3),
                     fit4 = coef(fit4))
row.names(output) <- names(coef(fit1))
```

```{r echo = printAnswers, eval=printAnswers}
output
```

---

3. **Inspect the estimates for the four models in the `output` object. What do you conclude?**

```{r echo = printAnswers}
# These estimates are very similar. 
```

---

# Plotting the relation

---

4. **Plot the pair `(x1, y1)` such that `y1` is on the Y-axis and make the color of the points blue**
This is quite simple to do with `ggplot2`
```{r}
anscombe %>%
  ggplot(aes(x = x1, y = y1)) + 
  geom_point(color = "blue")
```

In the above code we put the aesthetics `aes(x = x1, y = y1)` in the `ggplot()` function. This way, the aesthetics hold for the whole graph (i.e. all `geoms` we specify), unless otherwise specified. Alternatively, we could specify aesthetics for individual `geom`'s, such as in
```{r}
anscombe %>%
  ggplot() + 
  geom_point(aes(x = x1, y = y1), color = "blue")
```

We can also override the `aes(x = x1, y = y1)` specified in `ggplot()` by specifying a different `aes(x = x2, y = y2)` under `geom_point()`. 
```{r}
anscombe %>%
  ggplot(aes(x = x1, y = y1)) + 
  geom_point(aes(x = x2, y = y2), color = "blue")
```

---

5. **Plot the four pairs of variables from Exercise 1 in a single plotting window. Make the points in the plots `blue`, `gray`, `orange` and `purple`, respectively. **

In other words, create the following plot:
```{r echo = printAnswers}
gg <- anscombe %>%
  ggplot() + 
  geom_point(aes(x = x1, y = y1), color = "blue") + 
  geom_point(aes(x = x2, y = y2), color = "gray") + 
  geom_point(aes(x = x3, y = y3), color = "orange") + 
  geom_point(aes(x = x4, y = y4), color = "purple") +
  ylab("Y") + xlab("X")

gg
```

---

6. **Add a regression line to the plot for only the pairs `(y3, x3)` and `(y4, x4)` where the line inherits the colour from the respective points.**
Hint: use `geom_smooth()`.
```{r echo = printAnswers}
gg + # take the plot under #5 as the starting point 
  geom_smooth(aes(x = x3, y = y3), method = "lm", se = FALSE, color = "orange") + 
  geom_smooth(aes(x = x4, y = y4), method = "lm", se = FALSE, color = "purple")
```

---

7. **Now add a loess line to the plot from `Exercise 5` for *all pairs but* `(y4, x4)` where the line inherits the colour from the respective points.**
```{r echo = printAnswers}
gg + # take the plot under #5 as the starting point 
  geom_smooth(aes(x = x1, y = y1), method = "loess", se = FALSE, color = "blue") + 
  geom_smooth(aes(x = x2, y = y2), method = "loess", se = FALSE, color = "grey") +
  geom_smooth(aes(x = x3, y = y3), method = "loess", se = FALSE, color = "orange") 
```

---

# Assumptions

---

## Inspect `fit1`

---

8. **Inspect the assumptions for the first model. What do you think?**

HINT: use `plot()` and use the plots you've created in exercises 5-7.

```{r}
plot(fit1)
```

- The data follows a linear trend, although the loess curve shows some deviations from linearity. All in all, there are only 11 points, so this slight deviations is not something that would worry me. 
- Taking into account that there are only a few observations, I would argue that the residuals seem normally distributed from the `Normal Q-Q` plot. 
- The residual variance seems constant over the level of the fitted values (i.e. homoscedastic residual variance) as seen in `Residuals vs. Fitted` plot and the `Scale-Location` plot. Again, the dip in the `Scale-Location` plot can easily be explained by the small sample size and the deviation should be taken with a grain of salt. 
- No special remarks with respect to leverage and cook's distance, although case #3 would need to be at least investigated. 

---

## Inspect `fit2`

---

9. **Inspect the assumptions for the model `fit2`. What do you think?**

```{r echo=printAnswers2, eval = printAnswers2}
#- The data does not follow a linear trend, the deviation would definitely worry me. 
#- The residuals seem non-normally distributed, especially in the tails from the `Normal Q-Q` plot. 
#- I could not still argue that the residual variance seems more-or-less constant over the level of the fitted values. The residual variance is heteroscedastic.  
#- Case 8 has quite some leverage and a large residual. It's cook's distance is greater than `.5`. 

plot(fit2)
```

---

## Inspect `fit3`

---

10. **Inspect the assumptions for the model `fit3`. What do you think?**

```{r echo=printAnswers2, eval = printAnswers2}
#- The data follows a perfect linear trend, except for case #3 
#- The residuals seem normally distributed, except for case #3
#- I could not still argue that the residual variance seems more-or-less constant over the level of the fitted values. The residual variance is heteroscedastic. However, if case #3 were omitted, there are no residuals: every point falls perfectly on the regression line. 
#- Case 3 has quite some leverage, but not as large as other cases. Case #3 has the largest residual. It's cook's distance is greater than `1`. 

plot(fit3)
```

---

## Inspect `fit4`

---

11. **Inspect the assumptions for the model `fit4`. What do you think?**

```{r echo=printAnswers2, eval = printAnswers2}
#- The data follows no trend. You'd be an idiot to perform linear regression on this set. 
#- The residuals seem normally distributed
#- I could not still argue that the residual variance seems more-or-less constant over the level of the fitted values. The residual variance does not exist for fitted values other than 7!
#- Case 8 has a leverage of 1; hence it is omitted from most of the plots. The plot over the remaining points is redundant.  

plot(fit4)
```

